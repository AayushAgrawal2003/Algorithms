# Knowledge distillation 

it refers to the process of transferring the knowledge from a large unwidely model or a set of models to a single smaller model. 
- Makes it deployable
- Small student model learns to mimic a larger teacher model 
- similar or higher accuracy 
- 
-  
